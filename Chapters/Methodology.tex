%!TEX root = ../Thesis.tex
% Chapter Template

\chapter{Methodology} % Main chapter title

\label{Methodology} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

\lhead{Chapter \ref{Methodology}. \emph{Methodology}} % Change X to a consecutive number; this is for the header on each page - perhaps a shortened title

\color{red}TODO: The methodology and testing chapters are somewhat intertwined. The incrementally optimized parameter set is listed under algorithmic design in the methodology chapter, but how this parameter set is derived is not explained fully before the testing and evaluation chapter. Perhaps the incremental test section in the test chapter should be moved to the pre-experimental planning section in this methodology chapter.

The methodology chapter should be written in past tense?

\color{black}

Research methodologies and standards used to test search and classification algorithms will make the foundation of this thesis' research methodology. While experiments in the information retrieval field do not necessarily directly involve human subjects, there are still standards and methodologies in place to ensure that experimental results are valid. This chapter will describe the scientific approach used in the scientific field of information retrieval, and how this approach will be applied to this thesis work.

\section{Experimental Evaluation}
\label{ExperimentalEvaluation}
The configuration and parameter sets for the \CTC algorithm will be evaluated experimentally rather than analytically for reasons explained by \cite[p. 32]{Sebastiani2002}:
\begin{quote}The reason is that, in order to evaluate a system analytically (e.g., proving that the system is correct and complete), we would need a formal specification of the problem that the system is trying to solve (e.g., with respect to what correctness and completeness are defined), and the central notion of TC [Text Classification] (namely, that of membership of a document in a category) is, due to its subjective character, inherently nonformaliable. The experimental evaluation of a classifier usually measures its effectiveness (rather than its efficiency), that is, its ability to take the right classification decisions.
\end{quote}

Experimental evaluation have long been used and discussed in the field of information retrieval. One of the first experimental paradigms in information retrieval research, one that is still in use today, is the test collection evaluation paradigm introduced by The Cranfield research projects during the 60s \cite{Cleverdon1966}. The experimental methodology formed during these experiments are nicely summarized by \cite[p. 564]{Voorhees2005} who writes that:

\begin{quote}
At the core of this experimental methodology was the idea that live users could be removed from the evaluation loop, thus simplifying the evaluation and allowing researchers to run in vitroâ€“style experiments in a laboratory with just their retrieval engine, a set of queries, a test collection, and a set of judgments (i.e., a list of relevant documents).
\end{quote}.

\cite[p. 33]{Cleverdon1966} give three requirements for using measurements of performance in experimental tests of information retrieval systems:
\begin{enumerate}
\item A document collection of known size to be used in the test;
\item A set of questions, together with decisions as to exactly which documents are relevant to each question;
\item A set of results of searches made in the test; these usually give the numbers of documents retrieved in the searches, divided into the relevant and non-relevant documents.
\end{enumerate}
These questions should be asked with regards to information retrieval experiments done on text search via queries, but can inspire similar questions for experiments done on text classification and clustering algorithms. Instead of forming questions and determining which documents are relevant to those question, one can form a set of categories and then decide which documents fall into which categories. Then clustering results can be divided into clusters, each cluster correct or incorrect.

\section{Corpora}
\label{Corpora}

When performing experimental research on information retrieval systems it is customary to use standard document collections or corpora as they are also known. There are several corpora available for text classification and clustering research. \cite{Baeza-Yates2011a} describe some of the corpora available for text classification research among them: Reuters-21578, RCV: Reuters Corpus Volumes, the OHSUMED reference collection and 20 NewsGroups. Some of them are briefly explained below.

Reuters, an international news agency have made several corpora that are available trough different sources. One Reuters corpus that have been much used in the text classification community is the \textit{Reuters 21578} corpus \cite{Lewis2004a}. The documents in this collection was collected from documents appearing on the Reuters newswire in 1987. The corpus was assembled and categorized by personnel from Reuters and Carnegie Group in 1987. This corpus thus resembles that used in the ``Recycling of news in the news papers 1999 - 2000'' research project.

Reuters have since made a new corpus that is likely to replace the Reuters 21578 corpus. This new corpus is divided into three volumes RCV1, RCV2 and TRC2. The first two volumes contain news stories from 96 - 97, and the last volume contains news stories from 08 to 09. RCV1 and TRC2 contain English news stories only, while RCV2 is multilingual \cite{NationalInstituteofStandardsandTechnology2004}. An article on use of the RCV1 corpus provide more details about how the data set can be used in evaluation text categorization systems. ``\textit{Reuters CorpusVolume I (RCV1) is an archive of over 800,000 manually categorized newswire stories recently made available by Reuters, Ltd. for research purposes. Use of this data for research on text categorization requires a detailed understanding of the real world constraints under which the data was produced.}'' \cite{Lewis2004}. 

This thesis will mainly focus on news corpora, but the LLI research group will also look at the blogosphere. With this in mind it would be interesting to use a standard blog collection for evaluation of the algorithm in future research. Two blog collections are used in the blog track in the TREC conference, the Blogs06 and Blogs08 collections.\begin{quote}
The TREC Blogs06 collection is a big sample of the blogosphere, and contains spam as well as possibly non-blogs, e.g. RSS feeds from news broadcasters. It was crawled over an eleven week period from 6th December 2005 until the 21st February 2006. The collection is 148GB in size [\dots] The collection was used in TREC 2006, 2007 and 2008 \cite{Macdonald2011}.
\end{quote} 

These corpora form a solid foundation for experimental evaluations and make it possible to replicate and compare results between research projects and researchers. But for this to be possible, it is necessary to use some formal evaluation measures that are employed by a majority of the research in the area of study. This will be the focus of the next section (Section~\ref{EvaluationMeasures}).

\section{Evaluation Measures}
\label{EvaluationMeasures}
There are some considerations to take when choosing evaluation measures. When performing experimental research it is important to use the same evaluation measures as those used in related research works to make the results comparable. In much of information retrieval research, text classification included, there are agreed upon measures that can be used while performing research. Such measures does not exist to the same extent for clustering algorithms. There seems to be a community of practice with regards to evaluation measures used for the \STC algorithm. An alternative evaluation measure used by the LLI group in their research will be compared to the other measures.

\subsection{Chim and Deng}

\cite{Chim2007} detail how they perform an experimental evaluation of their clustering result in some detail. \citeauthor{Chim2007} use the evaluation measures on the results from a hierarchical clustering algorithm. The results from hierarchical clustering algorithms and the \CTC algorithm are however similar in nature. and it will be possible to use measurement formulas described in their article when calculating the various performance measurements. These formulas have also been used by \cite{Rafi2011} when they compare the standard suffix tree clustering algorithm of \citeauthor{Oren1998} with the algorithm formulated by \citeauthor{Chim2008}. Their papers describe four standard measurements for clustering quality: precision, recall, F-Measure and overall F-Measure.

If you have the sets

\begin{displaymath}
C = \{C_{1}, C_{2}, \dots, C_{k}\}
\end{displaymath}
\begin{displaymath}
C^* = \{C_1^*, C_2^*, \dots, C_l^*\}
\end{displaymath}
\begin{displaymath}
D = \{D_{1}, D_{2}, \dots, D_{k}\}
\end{displaymath}

where \(C\) is the clusters produced by the algorithms on document set \(D\), and \(C^*\) is the ``correct'' classes of document set \(D\), then the recall, precision and F-measure of cluster \(j\) with respect to class \(i\) can be calculated as:

\begin{displaymath}
recall(i,j) = \frac{\vert C_{j} \cap C_i^* \vert}{\vert C_i^* \vert}
\end{displaymath}
\begin{displaymath}
precision(i,j) = \frac{\vert C_{j} \cap C_i^* \vert}{\vert C_{j} \vert}
\end{displaymath}
\begin{displaymath}
F-Measure(i,j) = \frac{2 \cdot precision(i,j) \cdot recall(i,j)}{precision(i,j) + recall(i,j)}
\end{displaymath}

\cite{Chim2007} do not provide any information as to whether the category set \(C^*\) is disjoint (i.e. whether one document can occur in several categories). The category set is therefore most likely not disjoint, or it depends on the collection. As was explained in the literature section recall aims to capture the fraction of positive results to the total number of correct results. In this context recall expresses the fraction of a category's documents the cluster contains. Precision shows the fraction of documents in a cluster that is correctly clustered given a category to the amount of documents in a cluster. Because precision and recall is not good measures by themselves (recall could be a 100\%, but often precision would then be very low) an F-Measure is often used in evaluation of text classifiers \cite{Baeza-Yates2011a}. The F-Measure is the harmonic mean between recall and precision and is high when both precision and recall is high \cite{Baeza-Yates2011b}. 

The precision, recall, and F-Measure measurements defined above are applied to one cluster and class at a time. In other words the F-Measure of a cluster j with regards to a class i might not be any good, but its F-Measure with regards to another class i' might be very good. \cite{Chim2007} define an overall F-Measure function that captures the F-Measures for all the correct classes defined for the document set. For this function only those clusters j which maximize the F-Measure score for class i are considered in the overall F-Measure score. The overall F-Measure is calculated using the function:

Given the formulas the overall F-Measure of the clusters \(C\) can be calculated using the formula:
\begin{displaymath}
F := \sum_{i=1}^{l} \frac{\vert C_i^* \vert}{\vert D \vert} \cdot \max_{j=1,\dots,k} \{F-Measure(i,j)\}
\end{displaymath}

\subsection{LII research group}
The ``Klimauken'' corpus is tagged with five topics (tags or classes) per document rather than a single topic. See the tags attribute in Listing~\ref{lst:snippetfile} for an example of a document in snippet format. In context of this corpus, a ground truth cluster is defined as all those documents that have all five topics in common. But only looking at perfect matches, TODO: CITATION papers says, might exclude those clusters that closely match a ground truth cluster. In thus makes sense to talk about degrees of correctness. 

\subsubsection{Ground truth}

The discrepancy of a cluster \(C\) with regards to a ground truth cluster \(G\) is defined as \(5 - d\) where \(d\) is the number of tags in common in the set \(C_{tags} \cup G_{tags}\). If all documents in the cluster and ground truth cluster have all five topics in common, there is no discrepancy. Further we impose a limit on discrepancy, namely that \(C\) matches ground truth with discrepancy \(d\) if and only if there is a \(G\) such that:

\begin{displaymath}
\{ G \vert G \subseteq C \textit{ and } G = max(d(G_{1}, \dots, d(G_{n})))
\end{displaymath}

The discrepancy of a cluster in relation to the ground truth is thus its discrepancy to the best matching ground truth cluster. An example of a cluster is given in Table~\ref{tab:clusterexample}.

With this definition in place we understand that the number of ground truth cluster of zero discrepancy divided by the number of retrieved clusters act as the standard precision measure. The division of retrieved clusters into levels of discrepancy allows us to relax the original measure and for example allow ground truth clusters of discrepancy zero and one to be used when calculating precision.

\subsubsection{Ground truth represented}

Ground truth clusters represented is essentially a measure of recall. The ground truth represented value, \(GtR\), for ground truth clusters \(G\) of discrepancy \(d\) given \(C\) retrieved clusters is calculated: \(GtR = \frac{\vert G_{d} \vert}{\vert C \vert}\). Again only looking at ground truth clusters with a discrepancy of zero gives us the standard measure. Again one can use the accumulated recall values of discrepancy levels zero and one to include mostly correct clusters as ground truth clusters.

\subsubsection{Tag Accuracy}
TODO CITATION also introduce a measure called tag accuracy. This measure leaves behind the notion of ground truth all together in favour of measuring the coherency between the tags in the documents in the resulting clusters. \textit{``A cluster has a tag accuracy of 5 - d if and only if d is the number of words common to all tags in it.''} TODO: Citation. Given two documents with the tags 
\begin{inparaenum}[\itshape 1\upshape)]
	\item Innenriks-ulykker-akeulykke-8Ã¥ring-nordkapp, and
	\item Innenriks-kriminalitet-trafikk-fÃ¸rerkort-kristiansand,
\end{inparaenum}
we get a tag accuracy of 4.


\begin{table}[htdp]
\footnotesize
%\rowcolors{1}{gray!10}{white}
\begin{center}
\begin{tabular}{|c|p{10cm} |}
\hline
Label &  smÃ¸rer Odd-BjÃ¸rn \\
Topics overlap & 4 \\
Topics &  Nordmenniutland-sport-langrenn-hjelmeset-slovenia Nordmenniutland-sport-langrenn-hjelmeset-slovenia Nordmenniutland-sport-langrenn-verdenscup-slovenia \\
No. documents & 3 \\
Documents & www.vg.no/sport/ski/langrenn/artikkel.php?artid=581044 www.aftenposten.no/nyheter/sport/skisport/article3431048.ece www.adressa.no/sport/article1423419.ece \\
\hline
\end{tabular}
\end{center}
\caption{A cluster example}
\label{tab:clusterexample}
\end{table}


\begin{table}[htdp]
\footnotesize
%\rowcolors{1}{gray!10}{white}

\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
Topic overlap &  Fraction of total clusters & Percentage  & accumulated\\ 
\hline
0 -precision & 253 / 457 & 0.554 & 0.554 \\
1 -precision & 2 / 457 & 0.004 & 0.558 \\
2 -precision & 1 / 457 & 0.002 & 0.560\\
3 -precision & 2 / 457 & 0.004 & 0.565\\
4 -precision & 8 / 457 & 0.018 & 0.582\\
5 -precision & 191 / 457 & 0.418 & 1.000\\
\hline
\end{tabular}
\\Total: 457 (of  457)
\end{center}
\caption{Precision of cluster result}
\label{tab:clusterprecision}
\end{table}


\subsection{Measurement comparison}
Compare the measurements here
TODO: Find out differences...

\section{Experimental Research}
\label{ExperimentalResearch}

In order to answer the formulated hypotheses a scientific experiment must be formulated. This section will describe scientific experiments in context of algorithm analysis and outline an experiment design to test the hypotheses formulated to test the \CTC algorithm. The experiment will be formed around some of the techniques described in \cite{Bartz-Beielstein2004}, but with significant adjustments. In his article, \citeauthor{Bartz-Beielstein2004} is primarily concerned with the optimization of a particle swarm optimization algorithm. The techniques are non the less relevant as they provide valuable insight into experimental benchmarking of algorithms. The process is summarized in Table~\ref{tab:experimentsequence}.

\begin{table}[htdp]
\footnotesize
%\rowcolors{1}{gray!10}{white}
\caption{``\textit{Sequential approach. This approach combines methods from computational statistics and exploratory data analysis to improve (tune) the performance of direct search algorithms.}'', from \protect \cite[p. 417]{Bartz-Beielstein2004}}
\label{tab:experimentsequence}
\begin{center}
\begin{tabular}{|c|p{10cm}|}
\hline
Step & Action\\
\hline
(S-1) & Preâ€“experimental planning\\
(S-2) & Scientific hypothesis\\
(S-3) & Statistical hypothesis\\
(S-4) & Specification\\
(a) & optimization problem,\\
(b) & constraints,\\
(c) & initialization method,\\
(d) & termination method,\\
(e) & algorithm (important factors),\\ 
(f) & initial experimental design,\\ 
(g) & performance measure\\
(S-5) & Experimentation\\
(S-6) & Statistical modeling of data and prediction\\
(S-7) & Evaluation and visualization\\
(S-8) & Optimization\\
(S-9) & Termination: If the obtained solution is good enough, or the maximum number of iterations has
been reached, go to step (S-11)\\
(S-10) & Design update and go to step (S-5)\\
(S-11) & Rejection/acceptance of the statistical hypothesis\\
(S-12) & Objective interpretation of the results from step (S-11)\\
\hline
\end{tabular}
\end{center}
\end{table}

\subsection{Pre-experimental planning}
The first step is pre-experimental planning. During the pre-experimental planning tests were performed to find candidate parameters for the algorithm as well as good parameter ranges (see Chapter~\ref{EvaluationTesting}). Additionally the corpus file was processed to get a suitable format for clustering with different parameters. This included marking sentences up with appropriate text types.

\subsection{Scientific Hypotheses}

To investigate whether the new optimal parameter set performs better than the default and whether the automated optimization algorithm is effective the following formal hypotheses have been defined:


\begin{description}
	\item []Optimal parameter set
	\begin{description}
	\item [\(H1_{1}\)] There is a parameter set \(p_{optimized}\) for the \CTC algorithm that for the ``Klimauken'' corpus give significantly improved results compared to the parameter set \(p_{default}\).
	\item [\(H1_{0}\)] The parameter set \(p_{optimized}\) does not give significant improvements compared to the parameter set \(p_{default}\) for the ``Klimauken'' corpus.
	\end{description}
\end{description}

\begin{description}
	\item []Optimization algorithm
	\begin{description}
	\item [\(H2_{1}\)] The genetic algorithm described in this thesis produce a parameter set \(p_{optimized}\) that is significantly better than the default parameter set \(p_{default}\).
	\item [\(H2_{0}\)] The genetic algorithm described in this thesis does not produce a parameter set \(p_{optimized}\) that is better than the default parameter set \(p_{default}\).
	\end{description}
\end{description}

\(H2_{1}\) and \(H2_{0}\) only test the specific implementation of the Genetic Algorithm as implemented in this thesis work. Other optimization algorithms or other implementations of the Genetic Algorithm might perform differently from this one. By constricting the hypothesis to only account for this implementation of the Genetic Algorithm it is possible to formulate a null hypothesis.

\begin{description}
	\item []Genetic versus incremental search
	\begin{description}
	\item [\(H3_{1}\)] The genetic algorithm described in this thesis produce a parameter set \(p_{optimized}\) that is significantly better than an iteratively improved parameter set \(p_{iterativ}\).
	\item [\(H3_{0}\)] The genetic algorithm described in this thesis does not produce a parameter set \(p_{optimized}\) that is better than an iteratively improved parameter set \(p_{iterative}\).
	\end{description}
\end{description}

\color{red}For the optimized parameter set to be considered significantly better it should perform at least 5\% better in terms of the balanced F-Measure score. So for the optimized parameter set to perform significantly better than incremental improvement, if the incremental improvement yield an F-Measure of 0.5, then the optimized parameter set need to yield an F-Measure of at least 0.6.
\color{black}

The scope of this thesis does not allow for additional hypotheses to be investigated. For future research an additional possible hypothesis that would be worth testing is whether the optimized parameter set is also optimized for similar corpora.

\subsection{Specification of Experiment}
The specification of an experiment on the optimization of an algorithm should, according to \citeauthor{Bartz-Beielstein2004}, include a specification of the optimization problem, definition of constraints on the optimization, a description of how the algorithm is initialized, and an experiment design which describes the problem to be investigated and the algorithm design.


\subsubsection{Experiment Design}
The algorithm design describes the parameter sets used in the experimentation phase. There are three algorithm designs that will be used for the experiment. They are listed in Table~\ref{tab:algorithmdesign}. Each column corresponds to one parameter. The first algorithm design is labeled Etzioni. It is an approximation of the parameter set used by \citeauthor{Oren1998} in the article \citetitle{Oren1998}. Because the authors use different types of corpora and does not provide source code for an implementation of the algorithm some of the parameter chosen for the algorithm design might not accurately represent the original algorithm design.

The second algorithm design was derived from the incremental tests. The parameters represent the highest performing value for that parameter given either the algorithm design of \citeauthor{Oren1998} or the design of \supervisor. Alternatively an incrementally optimized parameter set could be derived by serially optimizing the parameters. I.e. optimize the first parameter, keep the optimized value, then optimize the next parameter and so forth. This method would however present difficulties of its own as the order in which the parameters are optimized would affect the result.

The third algorithm design was derived by running a genetic optimization process on the parameter set. TODO: Insert info about how the algorithm was run here.

\begin{landscape}
\begin{table}
\small
\begin{center}
  \begin{tabular}{|p{1.5cm}|p{1.5cm}|p{1cm}|p{1.5cm}|p{1.5cm}|p{1.5cm}|p{1.5cm}|p{1.5cm}|p{1.5cm}|p{1cm}|p{1.5cm}|p{1.5cm}|p{1.5cm}|}
  \hline
  Design & TT & TBC & MTO & MTR & MiLBCS & MaLBCS & DSBC & DOWC & OD & TA & TTy & SM\\ 
  \hline
  Etzioni & Suffix & 500 & 4 & 0.4 & 2 & 7 & False & False & True & 0 & Front page, headings, byline & Etzioni (0.5 threshold) \\
  \hline
  Incremental & Suffix & 9000 & 200 & 0.2 & 15 & 17 & False & True & False & 0.05 & Front page & Cosine (0.6, 0.5, 0) \\
  \hline
  Genetic & 4-slice & 7937 & 141 & 0.47 & 11 & 17 & False & True & False & 0.32 & Front page intro, Article heading, article intro & Etzioni (0.99) \\
  \hline
  \end{tabular}
\end{center}
\caption{The three algorithm designs used in the experiments.}
\label{tab:algorithmdesign}
\end{table}
\end{landscape}

Table~\ref{tab:problemdesign} describes the clustering problems used for the experiments. The Experiment column specify the hypothesis for which the problem design was applied. The algorithm design is a reference to the parameter sets specified in Table~\ref{tab:algorithmdesign}. To keep the results in line with the classifications provided by the experts that prepared the ``Klimauken'' corpus singleton ground truth clusters were preserved. The F-Beta constant was set to 1 to weigh precision and recall equally. Each experiment was run on the ``Klimauken'' corpus.

\begin{table}
\small
\begin{center}
  \begin{tabular}{|l|l|l|l|l|}
  \hline
  Experiment & Algorithm Design & Drop singelton ground truth & F-Beta constant & Corpus\\ 
  \hline
  H1 & Etzioni, Incremental & False & 1 & Klimauken\\
  \hline
  H2 & Etzioni, Genetic & False & 1 & Klimauken\\
  \hline
  H3 & Incremental, Genetic & False & 1 & Klimauken\\
  \hline
  \end{tabular}
\end{center}
\caption{The problem designs describing the clustering problem for the experiments.}
\label{tab:problemdesign}
\end{table}